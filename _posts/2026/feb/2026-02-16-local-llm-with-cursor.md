---
title: "Local LLM과 cursor..."
tags: [life]
layout: post
author: "Keith"
---

AI 학습과 활용을 위해 작년에 구매한 5070 Ti가 한동안 ‘계륵’처럼 느껴져 답답했는데, 최근 Qwen3를 올려보니 비로소 AI 하드웨어다운 면모를 보여주고 있다. 지난 OpenClaw 실험 때에서 허술한 모델을 올렸다가 실망했던 것과는 확연히 다른 체감이다.

현재 사용가능한 고사양 GPU로 4090도 테스트해본 바 VRAM 용량의 한계로 Qwen3 대형 모델을 구동하기엔 역부족임을 깨달았다. 5070 Ti와 비교해도 다룰 수 있는 모델 파라미터 크기에서 극적인 차이를 만들기 어렵다. 추론 속도와 성능 유지를 위해 KV cache나 컨텍스트 윈도우 할당량 등 VRAM 일부를 희생해야 하기 때문이다. 결국 4090은 5070 Ti보다 연산 속도가 제법 빠르다는 점 외에는 큰 변별력이 없었다.

일각에서는 5090의 성능을 높게 평가하지만, VRAM 크기가 획기적으로 늘어나지 않는 이상 한계는 여전하다. 다만 30B 모델을 4-bit 양자화(Quantization)하면 간신히 돌릴 수 있는 수준이라 들었다. 현재 5070 Ti에서 17B 모델을 추론용으로 사용하면 간혹 속도가 더디게 느껴지기도 하지만, 일반적인 온라인 챗봇 수준의 속도는 확보된다. Cursor와 연동해 사용해 보니, 유료 온라인 모델보다 응답은 느릴지언정 내가 직접 작업하는 것보다는 훨씬 빠르다. 작업을 지시해 두고 다른 일을 처리할 수 있다는 점에서도 활용 가치는 충분하다.

동료에게 업무를 지시할 때 소요되는 소통 비용과 시간, 결과물의 기복을 고려하면, 다소 느리더라도 일관되게 압도적인 결과물을 내놓는 로컬 LLM의 효용성을 인정하지 않을 수 없다. 이건 좋게 말해서 그렇지 그 답답한 인간들과 같이 일은 한답시고 일을 맡겨놓고 예상대로 허접한 결과를 받고 결국에 내가 짜증내며 다시 해야 하는 것과는 절대 비교할 수가 없다.

실제로 쓸만한 모델을 가지고 작업해보니 RAG의 도움을 받아 로컬 LLM 챗봇을 운용하거나, 번거로운 수작업을 말로 설명해 crontab처럼 자동화하는 과정에서 높은 만족감을 느낀다. API 호출 방식보다 로컬 환경을 선호하게 된 이유다. 아주 복잡한 과업이 아니라면 로컬 LLM만으로도 충분히 준수한 결과를 얻을 수 있다는 실무적 확신이 생겼다.

비용 측면에서 보면 5090급 GPU가 탑재된 고성능 PC 한 대 값으로 DGX Spark 같은 대안을 고려해 볼 만하다. 오로지 LLM 전용 머신으로만 활용한다면 나쁜 선택이 아니다. 고성능 PC는 게임이나 고속 ML 학습에는 유리하지만 대용량 모델 구동에 제약이 있고, 반대로 GB10 기반 시스템은 대용량 모델을 올릴 순 있지만 연산 속도가 5090에 비해 현저히 느리다는 단점이 있다. 전력 소모 또한 속도와 비례하므로 결국 효율과 성능 사이의 트레이드오프(Trade-off) 문제다.

결론적으로 하드웨어 선택에 따르는 고민은 여전히 깊지만, 개인이 이 정도 수준의 지능을 로컬에서 통제할 수 있다는 사실만으로도 세상이 비약적으로 발전했음을 부인하기 어렵다.