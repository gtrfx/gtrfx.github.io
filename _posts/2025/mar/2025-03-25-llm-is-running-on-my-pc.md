---
title: "Local LLM..."
tags: [life]
layout: post
author: "Keith"
published: false
---

컴퓨터에 관심이 많은 사람들 사이에서 자주 나오는 흥미로운 주제 중 하나는,
자신의 컴퓨터 자원으로 LLM을 직접 운용하거나 소규모로 학습시킬 수 있는가에 대한 문제다.

하루 종일 자료를 찾아본 결과, 내가 내린 결론은 이렇다.

#### 추론(Inference)은 가능하지만, 메모리가 핵심이다

LLM을 로컬에서 추론하려면 GPU 성능도 중요하지만,
무엇보다 GPU의 메모리 용량이 매우 중요하다.

요즘 GPU들은 아무리 고가 제품이라 해도
메모리 크기가 24~32GB 수준에 머무는 경우가 많아,
대형 LLM을 그대로 로딩하는 데 한계가 있다.
이 때문에 ‘가성비’ 논쟁이 생겨나는 것이다.


#### 학습(Training)은 더 까다롭다

대형 딥러닝 모델을 훈련하려면
FP 연산 속도가 빠른 GPU가 유리하다.
하지만 이러한 GPU들은 대부분 메모리 용량이 부족해서
학습 성능을 제대로 활용하기 어렵다.

결국, 본격적으로 무언가 해보려면
$2,500 이상의 지출은 각오해야 시작이 가능한 셈이다.


#### Mac vs Nvidia GPU: 선택의 갈림길

Mac (특히 Mac Studio나 M2 Ultra 기반 모델)은
GPU 코어 수와 메모리 용량이 넉넉해서
대형 LLM의 추론에는 적합할 수 있다.

하지만 여전히 학습 성능은 Nvidia GPU에 미치지 못한다.

반대로, Nvidia GPU(예: RTX 4090, 5090 등)는
학습 성능 면에서는 월등하지만
제품 수급이 원활하지 않고,
프리미엄 가격까지 부담해야 한다는 단점이 있다.


####  지금 할 수 있는 선택은 둘 중 하나

	1.	기다린다.
그동안은 GPU를 렌탈해서 필요한 만큼만 사용한다.
	2.	지금 바로 장비를 산다.
Mac Studio 또는 고성능 Nvidia GPU를 구입한다.
어떤 선택이든 분명한 장단점이 있다.

---

글쎄 성격 급한 나로서는 Mac Studio를 구입하고 training이 느린 건 나름 그냥 감수하는 방법? 을 선택하는 쪽에 강하게 끌리고 있다.