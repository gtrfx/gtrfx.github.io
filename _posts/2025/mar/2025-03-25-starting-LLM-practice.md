---
title: "LLM 맛이나 보기..."
tags: [life]
layout: post
author: "Keith"
---

재미삼아 LLM 맛보기로 간단한 Training을 해보았다. 대충 써놓고 chatgpt에게 만져달라고 했더니 이렇게 적어주었다. 잘 모르겠다. 원래 내가 이것을 쓰려고 한 것이었는지는.

나와 같이 컴퓨터를 좋아하는 옛날 사람의 눈으로 보면 뭐랄까, 사람의 언어를 sequence의 형태로 바꿔서 classifying/generation을 하는 놀이로 보여진다. 다만 이것을 통해서 엄청나게 많은 내용을 학습시키고 보니 그 쓰임새가 엄청나게 다양해져서 굉장한 의미를 갖게 된 것이랄까?

사실 학습 절차는 다음과 같은 단계를 거친다고 보면 된다:

1. **학습 데이터 수집**  
   - 일단 학습에 도움이 안 되는 문자들을 제거한다.

2. **토크나이징(Tokenizing)**  
   - 문자열을 그대로 신경망에 넣지 않고, 의미 있는 단어 또는 기호들을 숫자로 매핑한다.

3. **모델 구성**  
   - Multi-Head Attention 기반 모델을 만든다. (2017년 구글 논문 기반)

4. **학습 진행**  
   - 입력은 현재 토큰, 출력은 다음 토큰을 예측하는 방식.

> 결국 MLP로 매우 긴 시퀀스를 학습시키는 것이 그냥 묻지마 학습이었다면 여기에 attention(?)을 따지는 방법이 더해져 스마트해진 구조(?)랄까?

---

## 실제 실험

- 공개된 PyTorch 스크립트를 사용했다.
- 일반적인 classification MLP보다 훨씬 복잡한 구조.
- **짧은 연설문** 하나 학습시키는 데도:
  - `epoch = 10`
  - `16-core CPU 풀가동`
  - **10분 이상 소요**

> GPU를 사용했다면 훨씬 빨랐겠지만, 테스트 환경은 CPU-only였다.

---

## 모델 저장 및 포맷

- 매 epoch마다 `torch.save()`로 weight 저장.
- 저장 파일은 압축된 형태의 바이너리:
  - LZ 계열(LZ4로 추정)
  - 추가 압축해도 큰 변화 없음
  - 속도 중시로 복잡한 압축 방식은 사용하지 않은 듯

---

## CPU 부하 변화

- **Forward/Backward propagation 중엔 peak 부하**
- Python에서 부수작업 처리 시 load 감소
- 전형적인 학습 흐름 구조

---

## Sequence와 학습 부하

- 시퀀스 길이는 짧지만, 토큰 간 통계적 유사성 분석 필요
- Layer 수가 많아 연산량 많고 학습 시간 길어짐

> MATLAB에서 adaptive filter 훈련시키던 것과 비교해도 차원이 다른 부하다.

---

## 회상과 현실 인식

- 예전보다 컴퓨터 성능은 획기적으로 좋아졌지만
- **신경망의 복잡도는 그 이상의 속도로 증가 중**
- 상용 LLM 크기를 고려하면,  
  포터블 컴퓨터에서 돌리기까지는 시간이 걸릴 듯

하지만 추론용 weight를 단순화한 모델들을 보면  
그리 먼 미래는 아닐 수도 있다는 생각도 든다.

---

## 기술 발전의 두 열쇠

1. **곱셈/덧셈 + 데이터 전송 최적화**
2. **반도체 공정 기술의 발전**

> 지금 하는 작업은 결국 신경망이라는 장치에  
> 사람이 이해 가능한 정보를 **통계적으로 외우게 하는 것**에 가깝다.

---

## LLM을 바라보는 개인적인 정의

> 확률적 연관성을 바탕으로 다양한 패턴을 기억하고  
> 필요할 때 잘 꺼내 쓸 수 있도록 설계된 구조.  
>  
> "그냥 엄청나게 외우게 만든 거다."

---

## 현실적인 고민

- 간단한 연설문조차 GPU 리소스를 많이 소모
- 취미라 하기엔 **전력/시간 소비**가 상당
- **실수 한 번에 드는 비용**이 너무 크다

---

## 결론

> 그래서 사람들이 천문학적인 비용을 들여서 장비를 구입하는 모양이다. 
> 시간을 돈으로 사는 행위인 거다. 
> 내가 재미로 이 놀이를 하겠다면? 
> 잘은 몰라도 이것도 다른 아저씨들이 좋아하는 취미를 훨씬 능가하는 비용이 필요할 듯 하다.
> 장비빨을 세우기도 (글쎄 고가 장비는 개인이 구입할 수도 없을 듯 하다), 
> 또 전력이 꽤나 소모되는 데다가 
> 아주 작은 개념이라도 맛을 보는 데는 제법 많은 실험을 해야 하니 꽤나 시간이 들 것 같다. 


