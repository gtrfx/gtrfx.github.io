---
title: "Transformer..."
tags: [llm, aiml]
layout: post
author: "Keith"
published: false
---

요새 이것의 개념도 알고 학습도 시켜보고 또 학습된 모델을 fine tuning할 줄 알아야 한다는 둥의 얘기가 돌고 있어서 좀 찾아봤다.

## Attention이 뭘 뜻하는 건가?

어떤 것을 유심히 봐야하는가를 나타낸 값이라고 생각한다.

### Scaled-dot-attention

일단 이 안에는 3개의 내부 값이 있다. Q,K,V라고 해서 임의로 명명한 것 같다. 그렇게 명명하면 이해하기 쉬워서 그랬는지도 모르지만.

어쨌든 그 내부 값은 하나의 입력값 (문장? 시퀀스?)에서 출발한 거다. 다른 matrix를 곱해서 뽑아낸 값이야. 

그런데 Q,K를 서로 곱해서 유사도를 보고 이게 너무 큰 값이 되지 않게 적당히 스케일 (1/sqrt(dim))한 다음 softmax를 구해.

그러니까 결과값을 확률처럼 보이게 하는 거지. 그러니까 벡터 전체 값으로 element 개개의 값을 나눈 거야. 총합이 1이 되겠지.

그렇게 나온 결과를 다시 V에 곱해. 그것을 사람들이 scaled-dot-product-attention이라고 불러.

![attention](https://machinelearningmastery.com/wp-content/uploads/2022/03/dotproduct_1.png)


말 그대로 scale된 dot-product로 계산한 attention이다 이거지.

그 결과값은 V에 Q/K로 계산한 확률 값을 곱해놓은 거야.

명명한 대로 생각해보면, 

> '질문한 내용(Q)과 그것을 검색할 때의 key값(K)의 유사도를 가지고 그걸로 검색 내용(V) 중에서 쓸모있는 것만 골라내겠다.' 이런 의도로 읽혀.

다시 말해서 attention이라는 게 '입력에서 의미있는 부분만 부각해놓은 결과'라고 생각해.

### multi-head attention

이 계산 과정을 병렬로 만들어놓은 걸 multi-head attention이라고 한다. 한개의 attention 만으로는 뭔가 문제가 잘 안풀렸기에 이걸 확장해놓은 거라고 보여진다.

같은 연산과정을 (병렬로) 여러 개를 하게 하는 거지. 이게 training을 하면 어디로 수렴될지는 모르겠지만.

## Transformer

그림에서 보이는 구조가 transformer 되시겠다. 좌측이 encoder 우측이 decoder라고 한다. 

![transformer](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)

번역기를 개발하다가 나온 모델이라고 하는데, 쉽게 생각하면 encoder가 언어 중립적인 값을 뽑아오는 부분, decoder는 그 내용을 가지고 다른 언어로 만들어주는 부분으로 해석이 된다. 

### Encoder

잘 보면 attention의 결과를 FFN에 통과시킨 결과가 encoder의 출력이 된다. 이 attention이라는 것은 self-attention이다. 내가 입력한 문장안에서 토큰과 토큰간의 관계를 보고 attention을 계산하는 거다.

여기서 의문이 들 수 있다. 기계가 뭘 안다고 문장안에서 단어와 단어의 관계를 계산하고 하냐고. 여기서 이게 어떤 의미를 찾을 수 있느냐 등등. 

training이 완벽하게 끝난 상태에서 내부의 weight 값들이 존재하고 있다고 가정하면 이해가 좀 빨라질 것 같다. 원작자라고 해서 뭔가를 알았다기 보단 계속해서 이런 저런 방법으로 학습시켜보니 이런 구조가 잘 맞을 것 같다고 해서 만든 것일 것이다. 오죽하면 '모델'이라고 부르겠나.

사람이든 누구든 언어를 이런 식으로 학습하게끔 되어있다는 '사실'이 발견되었다면 모델이라고 부르지 않겠지만, 당장엔 언어를 어떤 식으로 학습해야 할지 모르지만 이렇게 하면 되지 않을까? 해서 만들어 냈기 때문에 '모델'인 거다.

수많은 데이터를 가지고 학습을 했다면 입력된 문장을 여러 가지 카테고리 별로 관찰해서 단어와 단어가 어떤 관계를 맺고 있는지 표현하는 값을 갖게 될 것이다.

이를테면 문장 내부에서 별다른 연관관계가 없다면 낮은 attention 결과가 나올 것이고, 문장 자체의 의미를 나타내는데 의미가 없는 단어라면 마찬가지로 낮은 attention을 갖는 것으로 학습될 것이다.

왜? 좀 있다가 생각해보자.

### Decoder

Decoder는 2개의 attention으로 구성된다. 1개의 self attention과 encoder단에서 가져온 결과를 참조하는 또 다른 attention으로 구성된다. 


### Output probability

그래서 출력의 self attention을 V에 넣어주고 encoder 출력으로 가져온 것을 Q/K에 넣은 것으로 attention을 또 한번 계산해서 그렇게해서 출력으로 확률 (softmax)을 얻어낸다.

### Input & Output

NLP에 대해 아무런 경험이 없으면 이게 뭔소리인가 할 수 있을텐데, 이 구조가 문장을 학습시키는 용도로 만든 거란 걸 생각하고 입력을 넣으면 예상되는 출력은 한 토큰 밀린 결과값이라는 것을 알아야 한다. 

Output에 보면 shifted right이라고 되어있는 말의 뜻이 오른쪽으로 (토큰) 하나 밀렸다라는 뜻이다. 

즉 입력을 넣었을 때 원하는 출력은 한 토큰 밀린 결과, 그러니까 transformer가 예측한 결과는 입력으로 들어간 것의 다음 단어/문장부호가 되야하는 것이다. 그렇게 학습을 시킨다.

### 모델이 어떻게 굴러가는 건가 그럼?

encoder는 입력 문장의 의미를 파악해서 그 의미에 대한 정보로 변환하고 decoder는 그걸 가지고 원하는 결과를 만들어내는 부분이라고 했다. 
decoder는 encoder의 출력을 받고 


### RNN

Transformer가 나오지 않았을 시절엔 NLP라는 게 문장을 토큰화하고 그것을 RNN에 밀어넣으면 다음에 나올 단어를 찾아내는 그런 일을 하는 걸 만드는 것이었다. 무슨 원리에서 그런지는 (본인들도) 잘 모르겠고 망 구조를 복잡하게해서 계속해서 트레이닝을 시키다보면 뭐라도 되겠지 하고 한 게 아닐까 싶은데. 

어쨌든 이 방식으로 가르치다보면 문장에서 단어와 단어의 관계를 학습시킨다거나 문장 내에서 중요한 요소가 뭔지 학습시키긴 쉽지 않았을 것 같다. 또 문맥이란 게 얼마나 오래 지속되는 것인지 알기도 힘들고. LSTM이나 GRU가 그 sequence의 문맥이 지속되는 것을 흉내내기 위해 나온 거라고 이해하는데, 실험할 때마다 '그때 그때 달라요' 하지 않았을까?

실험을 많이 해보지 않아서 잘은 모르지만 그렇다.

## 요약하면

- Transformer는 언어처리를 위한 학습 모델이다.
- 문장을 통째로 입력하는데, 의도하는 출력은 한단어가 밀린 입력이 된다. 그렇게해서 학습하게 된다.
- 문장 내에서 토큰과 토큰의 관계를 따져보는 게 attention이 되겠다. 문장내에서 '뭣이 중헌디'를 따져보는 값이 되겠다.
- 문장이 자체적으로 이 attention을 따지는 일은 Encoder에서도 한번하고 Decoder에서도 한번 한다.
- Decoder는 encoder의 출력을 가지고 또 한번 더 attention을 따진다. 
- 이걸 이용해서 RNN의 한계를 극복했단다.