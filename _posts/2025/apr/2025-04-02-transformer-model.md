---
title: "Transformer model..."
tags: [llm]
layout: post
author: "Keith"
---

Transformer model을 1주 가량 들여다보고 있다. 

어떤 새로운 개념을 이해하는데 이렇게 많은 시간을 들여보긴 처음이지만, 
학부시절 펑펑 놀다가 갑자기 대학원에 입학하고,    
새로운 분야의 논문을 읽고 이해해야 하던 시절이 생각나서 한편으로 꽤나 즐거운 경험이었다.

이게 transformer와 관련된 주변 사실들을 학습하고 이해갸 되면서 전반적으로 이해가 되는 측면이 있다.
그러니까, neural network로 뭔가를 학습시키고 인식/분석하는 실험들과 병행하면서 하다보면    
황당했던 개념들이 서서히 이해가 되고 받아들여지는 그런 기분이랄까.

이 개념은 그냥 내가 평소 알고 있는 어떤 시스템의 블록 다이어그램을 통해서 이해하는 것과는 좀 다르다. 

#### transformer

transformer라는 것이 처음 나온 것은 번역기를 만들면서 부터라고 한다. 논문을 보면 영어를 독일어로 번역하는 모델을 만들려고 하다 나온 것이란다.
여기에 나오는 전체적인 구조 (아래 그림)은 어떤 기본적인 개념을 학습하는데 너무 큰 노력이 들어간다고 본다. 

내가 가장 먼저 들은 이야기가 transformer가 생성형 AI의 시초가 되었고 그것은 encoder와 decoder로 구성되고.... 그래서 뭘 먼저 알고 이해해야하는지 혼돈이 일어난다.

![transformer](https://machinelearningmastery.com/wp-content/uploads/2021/10/transformer_1.png)

아래 그림은 이 구조에서 가장 중요한 부분이다. 일단 하나의 입력으로부터 3가지로 다르게 변환한 값을 가지고 처리하는데, 첫번째와 두번째는 서로 곱한 뒤에 어느 정도의 유사도가 있는지 확률로 계산하고 그것을 바탕으로 나머지 하나에 적용해서 확률상으로 높은 것은 키우고 나머지는 줄여주는 그런 일을 하는 것으로 구성된다.

이것을 scaled dot attention이라고 하는데, 일단 어떤 것을 중요하게 봐야하는지를 attention이라고 하고 그것을 dot product와 scale로 얻어냈기 때문에 scaled dot attention이라고 한다.

![Scaled Dot Attention](https://machinelearningmastery.com/wp-content/uploads/2022/03/dotproduct_1.png)

이것을 병렬로 여러 개로 확장을 시켜내면 그것을 multi-head attention이라고 한다. 그러니까 하나의 입력(=문장)에 대해서 이렇게도 보고 저렇게도 보고 해서 어떤 것을 강조해서 바라봐야 될지를 계산해 내는 것이다. 

다음은 이것을 이용한 GPT-2 model을 나타낸 것이다.

![GPT-2 Model](https://www.researchgate.net/publication/370853178/figure/fig1/AS:11431281159610040@1684426793285/Conceptual-architecture-of-a-GPT-model.ppm)

이 그림도 앞의 그림들과 비교하면 구조가 큰 차이가 없다. multi-head attention을 기본 요소로해서 돌아가도록 되어있다. 일반적으로 layer는 12개 head의 수를 12개로 해서 가져간다고 한다. 

#### 왜 이걸 하지?

가장 궁금한 것은 이걸로 뭘하지? 하는 것이다. 뭘 넣으면 뭐가 나오는 거지? 

이것은 일종의 (다음) 단어 예측기라고 해야 맞다.

그러니까, 토큰 (=단어 혹은 중요 문장부호)을 입력하면 다음 토큰을 예측해내는 장치인 것이다. 당연히 학습할 때 예상되는 정답은 한 토큰 다음의 입력이 된다. 그러니까 이 장치를 통해서 얻고자 하는 것은 어떤 맥락을 가진 sequence가 주어졌을 때 다음에 나와야 할 가장 높은 확률의 sequence를 예측하는 것이다. 

단어는 단어 그대로의 스트링이 들어가는 게 아니라 그것을 어떤 숫자에 매핑하고 (이걸 이들 용어로 embedding이라고 한다) 그것을 넣어준다. 결과로 나오는 것도 마찬가지로 단어를 의미하는 숫자가 된다. 그것을 토큰이라고 하는 것이다. 

학습과정에서는 망을 구성하는 요소들(linear weights)에 관찰하는 방향에 따른 유사성/상관관계들을 넣어주는 일을 한다. 추론할 때는 기존에 형성된 단어와 단어 간의 연관관계를 이용해서 가장 그럴싸 한 단어를 선택해서 내보내는 일을 하다보면 원하는 답을 내주는 기능하게 되는 것이다.

#### 파라미터의 양

일반적으로 파라미터의 양은 이 scaled dot attention을 계산하는데 들어가는 linear weight + bias와 그 다음으로 연결되는 feed-forward network의 weight/bias가 대부분을 차지한다. 이것을 얼마나 길게 볼 것이냐 얼마나 많은 layer를 놓고 볼 것이냐에 따라 파라미터의 양이 비례하게 된다. 

주로 LLM 학습용으로 쓰이는 GPT-2 model을 보면 12 head, 12 layer (of multi-head attention)이다.

대충 생각해봐도 하나의 scaled-dot-attention에 

#### 계산량을 생각해볼까?

그림에서 봐도 scaled-dot-attention 블록이 가장 많은 연산을 하게 된다. 또 이 블록이 가장 많은 개수로 들어가 있게 된다. 만약에 하드웨어로 만든다고 하면 이 부분을 얼마나 야무지게 만드느냐에 따라서 해당 학습/추론 머신의 성능이 결정될 것이다. 내가 이해하기로 deepseek model에서는 이 안에 들어가는 계산량을 줄이는 노력을 한 것으로 알고 있다 (나중에 알아보기로 하자). 불필요한 연산이나 메모리 액세스를 살짝 줄이는 것만으로 학습/추론 속도 증가와 엄청난 에너지 절감을 가져올 것이다.

개념적으로 구현되는 블록들에 대한 신호처리 하드웨어적인 최적화를 하면 지금처럼 (CUDA) GPU가 고생하는 것에 비해 훨씬 작은 수고로도 원하는 목표를 이룰 수 있을 것이다. 실제로 그런 노력이 이루어지고 있고. 